{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dea605e",
   "metadata": {},
   "source": [
    "# Phase 2 Model Validation\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Following the comprehensive analysis in Phase 1, `mobilenet_v2` was identified as the optimal architecture, uniquely satisfying our requirements for real-time inference speed, compact size, and strong accuracy potential.\n",
    "\n",
    "This notebook executes Phase 2 of our project. Instead of re-training the model from scratch, we will adopt a highly efficient strategy: we will acquire and validate a `mobilenet_v2` model that has already been fine-tuned on the full Food101 dataset by the community on the Hugging Face Hub.\n",
    "\n",
    "The primary goal is to perform a final, rigorous benchmark to confirm that this pre-trained model meets our production criteria before proceeding with deployment.\n",
    "\n",
    "Our investigation will proceed in two key steps:\n",
    "\n",
    "1/ Model Acquisition:\n",
    "> We will first download the selected pre-trained [mobilenet-finetuned-food101](https://huggingface.co/paolinox/mobilenet-finetuned-food101/tree/main) model and prepare the full Food101 test `DataLoader`.\n",
    "\n",
    "2/ Quick Validation:\n",
    "> We will then programmatically evaluate the model's final **accuracy**, **size**, and **inference speed** to ensure it aligns with our expectations and is ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e01f3b",
   "metadata": {},
   "source": [
    "## 00. Import base libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a123ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import count_loaded_model_parameters, benchmark_loaded_model_speed\n",
    "from src.engine import evaluate_model\n",
    "\n",
    "import warnings\n",
    "# The warnings from Hugging Face are informative but not critical for our task.\n",
    "# We can safely ignore them to keep the notebook output tidy.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2ff71",
   "metadata": {},
   "source": [
    "## 01. Model Acquisition\n",
    "\n",
    "In this section, we will prepare all necessary components for our final validation. This involves defining the Hugging Face Hub identifier for our chosen [mobilenet-finetuned-food101](https://huggingface.co/paolinox/mobilenet-finetuned-food101/tree/main) model.\n",
    "\n",
    "We will leverage the `transformers` library to handle the entire acquisition process. Its `AutoModel` and `AutoImageProcessor` classes will automatically download and cache the model's architecture, weights, and the specific preprocessing configuration required. This approach ensures perfect compatibility and reproducibility.\n",
    "\n",
    "Finally, to establish a robust ground truth for our benchmark, we will use the acquired image processor to create the **full Food101 test** `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de01b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data directory: ..\\data\\raw\n",
      "Local model cache directory: ..\\saved_models\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the root data directory\n",
    "DATA_PATH = Path(\"../data/raw\")\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {DATA_PATH}\")\n",
    "\n",
    "# Define the directory to save/cache our final model within our project structure\n",
    "SAVED_MODELS_PATH = Path(\"../saved_models\")\n",
    "SAVED_MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Local model cache directory: {SAVED_MODELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c9dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard label map not found. Creating one at: ..\\saved_models\\food101_label_map.json\n",
      "-> Successfully created and saved food101_label_map.json\n"
     ]
    }
   ],
   "source": [
    "# Define the path for our standard label map\n",
    "LABEL_MAP_PATH = SAVED_MODELS_PATH / \"food101_label_map.json\"\n",
    "\n",
    "# --- Create the standard Food101 label map IF it doesn't exist ---\n",
    "if not LABEL_MAP_PATH.exists():\n",
    "    print(f\"Standard label map not found. Creating one at: {LABEL_MAP_PATH}\")\n",
    "    \n",
    "    # Load the dataset just to get the class names in alphabetical order\n",
    "    temp_dataset = datasets.Food101(root=DATA_PATH, split=\"test\", download=True)\n",
    "    class_names = temp_dataset.classes # This is a list of 101 class names\n",
    "    \n",
    "    # Create a dictionary mapping from integer index to class name\n",
    "    # e.g., {0: \"apple_pie\", 1: \"baby_back_ribs\", ...}\n",
    "    label_map = {str(i): name for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(LABEL_MAP_PATH, \"w\") as f:\n",
    "        json.dump(label_map, f, indent=4)\n",
    "        \n",
    "    print(\"-> Successfully created and saved food101_label_map.json\")\n",
    "else:\n",
    "    print(f\"Found existing standard label map at: {LABEL_MAP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01d4f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model for validation: Haaaaaaaaaax/efficientnet-b3-finetuned-food101\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for the Chosen Model ---\n",
    "FINAL_MODEL_INFO = {\n",
    "    \"hub_id\": \"Haaaaaaaaaax/efficientnet-b3-finetuned-food101\",\n",
    "    \"local_dir_name\": \"Haaaaaaaaaax_efficientnet_b3_finetuned_food101\",\n",
    "}\n",
    "\n",
    "print(f\"Selected model for validation: {FINAL_MODEL_INFO['hub_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf52a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model not found at '..\\saved_models\\Haaaaaaaaaax_efficientnet_b3_finetuned_food101'.\n",
      "Downloading model and processor from Hub: Haaaaaaaaaax/efficientnet-b3-finetuned-food101...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db7355fddb74f8d8e0f95c99dc4764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and processor to our local project cache...\n",
      "Download and local caching complete.\n",
      "\n",
      "Loading model and processor from local project directory...\n",
      "-> Model and Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "# The full path to our local model directory\n",
    "local_model_dir = SAVED_MODELS_PATH / FINAL_MODEL_INFO['local_dir_name']\n",
    "hub_id = FINAL_MODEL_INFO['hub_id']\n",
    "\n",
    "# --- Step 1: Check for local project cache or download from Hub ---\n",
    "if not local_model_dir.exists():\n",
    "    print(f\"Local model not found at '{local_model_dir}'.\")\n",
    "    print(f\"Downloading model and processor from Hub: {hub_id}...\")\n",
    "    \n",
    "    # Use transformers to download the model and processor\n",
    "    model_from_hub = AutoModelForImageClassification.from_pretrained(hub_id)\n",
    "    processor_from_hub = AutoImageProcessor.from_pretrained(hub_id)\n",
    "    \n",
    "    # Now, save them to our own project's `saved_models` directory\n",
    "    print(f\"Saving model and processor to our local project cache...\")\n",
    "    model_from_hub.save_pretrained(local_model_dir)\n",
    "    processor_from_hub.save_pretrained(local_model_dir)\n",
    "    \n",
    "    print(\"Download and local caching complete.\")\n",
    "else:\n",
    "    print(f\"Found existing local model at: {local_model_dir}\")\n",
    "\n",
    "# --- Step 2: Load the model and processor FROM OUR LOCAL DIRECTORY ---\n",
    "print(\"\\nLoading model and processor from local project directory...\")\n",
    "\n",
    "final_model = AutoModelForImageClassification.from_pretrained(local_model_dir)\n",
    "final_model.to(device)\n",
    "final_model.eval()\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(local_model_dir)\n",
    "print(\"-> Model and Image Processor loaded successfully.\")\n",
    "\n",
    "# --- 3. Create a Transform Function and the Final DataLoader ---\n",
    "def create_transform(processor):\n",
    "    def transform_func(image):\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        return processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "    return transform_func\n",
    "\n",
    "final_transform = create_transform(image_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f14829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the full Food101 test dataset...\n",
      "-> Found 25250 images in the test set.\n",
      "-> Final test DataLoader created.\n",
      "\n",
      "--- Acquisition & Setup Complete ---\n",
      "The following variables are now ready for the validation section:\n",
      "1. The instantiated model: `final_model` (loaded from 'Haaaaaaaaaax_efficientnet_b3_finetuned_food101')\n",
      "2. The validation DataLoader: `test_dataloader`\n",
      "3. The standard label map path: `LABEL_MAP_PATH`\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Create the Dataset and DataLoader ---\n",
    "print(\"\\nPreparing the full Food101 test dataset...\")\n",
    "test_dataset = datasets.Food101(root=DATA_PATH,\n",
    "                                split=\"test\",\n",
    "                                transform=final_transform,\n",
    "                                download=True)\n",
    "print(f\"-> Found {len(test_dataset)} images in the test set.\")\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=32,\n",
    "                             num_workers=0,\n",
    "                             shuffle=False)\n",
    "print(f\"-> Final test DataLoader created.\")\n",
    "\n",
    "\n",
    "# --- Final Status ---\n",
    "print(\"\\n--- Acquisition & Setup Complete ---\")\n",
    "print(\"The following variables are now ready for the validation section:\")\n",
    "print(f\"1. The instantiated model: `final_model` (loaded from '{local_model_dir.name}')\")\n",
    "print(f\"2. The validation DataLoader: `test_dataloader`\")\n",
    "print(f\"3. The standard label map path: `LABEL_MAP_PATH`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084167f",
   "metadata": {},
   "source": [
    "## 02. Quick Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85209b40",
   "metadata": {},
   "source": [
    "With our `mobilenet_v2` model and the full test DataLoader now ready, we will perform a final, consolidated validation. Unlike the broad exploration in Phase 1, the goal here is to efficiently generate a definitive \"report card\" for our chosen candidate.\n",
    "Our streamlined validation process will programmatically compute our three key metrics and present them in a final summary table to confirm the model's suitability for deployment:\n",
    "\n",
    "- Performance: Calculate the top-1 accuracy on the full test set.\n",
    "- Efficiency (Size): Calculate the model's exact size in Megabytes.\n",
    "- Efficiency (Speed): Benchmark the average inference time on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca6df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating accuracy for 'Haaaaaaaaaax/efficientnet-b3-finetuned-food101' using the standard label map...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba085e5574e4478ae652f7d3e89b1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Accuracy:   0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- 1. Performance Validation: Calculating Accuracy ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCalculating accuracy for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINAL_MODEL_INFO[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhub_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using the standard label map...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlabel_map_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_MAP_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-> Final Validated Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ToanTo\\Desktop\\food101-classifier\\food101-classifier\\src\\engine.py:271\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataloader, device, label_map_path, verbose)\u001b[0m\n\u001b[0;32m    267\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    268\u001b[0m pred_labels_model_space \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    270\u001b[0m pred_labels_standard_space \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m--> 271\u001b[0m     [translation_map\u001b[38;5;241m.\u001b[39mget(label\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    272\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pred_labels_model_space],\n\u001b[0;32m    273\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    274\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m correct_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred_labels_standard_space \u001b[38;5;241m==\u001b[39m\n\u001b[0;32m    278\u001b[0m                   labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    279\u001b[0m total_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n",
      "File \u001b[1;32mc:\\Users\\ToanTo\\Desktop\\food101-classifier\\food101-classifier\\src\\engine.py:271\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    267\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(inputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    268\u001b[0m pred_labels_model_space \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    270\u001b[0m pred_labels_standard_space \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m--> 271\u001b[0m     [translation_map\u001b[38;5;241m.\u001b[39mget(\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    272\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pred_labels_model_space],\n\u001b[0;32m    273\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    274\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    277\u001b[0m correct_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred_labels_standard_space \u001b[38;5;241m==\u001b[39m\n\u001b[0;32m    278\u001b[0m                   labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    279\u001b[0m total_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. Performance Validation: Calculating Accuracy ---\n",
    "print(f\"\\nCalculating accuracy for '{FINAL_MODEL_INFO['hub_id']}' using the standard label map...\")\n",
    "final_accuracy = evaluate_model(model=final_model,\n",
    "                                dataloader=test_dataloader,\n",
    "                                device=device,\n",
    "                                label_map_path=LABEL_MAP_PATH,\n",
    "                                verbose=True)\n",
    "\n",
    "print(f\"\\n-> Final Validated Accuracy: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997079e3",
   "metadata": {},
   "source": [
    "## 03. Phase 2 summary & key takeaways"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
